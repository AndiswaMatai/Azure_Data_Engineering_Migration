# Azure Incremental Data Pipeline (Streaming-Style)

## Business Context
Full dataset reloads delayed analytics and increased compute costs. Incremental ingestion reduced latency and costs.

## Architecture Overview
- Ingestion: Azure Data Factory (watermark-based)
- Transformation: Azure Databricks
- Analytics: Azure Synapse
- Visualization: Power BI

# ADF incremental pipeline run
run_response = client.pipelines.create_run(
    rg_name, df_name, "IncrementalLoadPipeline",
    parameters={"WatermarkValue": "2026-01-01T00:00:00Z"}
)
print(f"Incremental pipeline run ID: {run_response.run_id}")

# Databricks transformation: deduplicate incremental data
mobility_df = spark.read.parquet("abfss://mobility@datalake.dfs.core.windows.net/raw/mobility_data")

incremental = mobility_df.filter(mobility_df.timestamp >= "2026-01-01")
deduped = incremental.dropDuplicates(["trip_id"])
deduped.write.mode("append").parquet("abfss://mobility@datalake.dfs.core.windows.net/curated/mobility_data")


-- Synapse SQL: Real-time mobility insights
SELECT Region,
       COUNT(DISTINCT trip_id) AS ActiveTrips,
       AVG(DurationMinutes) AS AvgTripDuration
FROM curated.mobility_data
WHERE timestamp >= DATEADD(hour, -1, GETDATE())
GROUP BY Region;

